{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use the same Artificial Neural Networks as in ANN(Deep and Wide with High Nodes Vs Low Nodes) but we will scale the dataset using MinMaxScaler from sklearn and also use the dataset with Z_Scores of Words and Helpful Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Stars', 'Helpful Votes', 'Z_Score_HelpfulVotes', 'Words',\n",
      "       'Z_Score_Words', 'Paragraphs', 'No.break tags', 'Percentage_Upper_Case',\n",
      "       'Percentage_Lower_Case', 'Avg_len_paragraph_per_review'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Z_Score_Words</th>\n",
       "      <th>Paragraphs</th>\n",
       "      <th>No.break tags</th>\n",
       "      <th>Percentage_Upper_Case</th>\n",
       "      <th>Percentage_Lower_Case</th>\n",
       "      <th>Avg_len_paragraph_per_review</th>\n",
       "      <th>Z_Score_HelpfulVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>6.453577</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>3087.000000</td>\n",
       "      <td>-0.235881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1.394079</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>0.915696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3.666459</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>90</td>\n",
       "      <td>468.500000</td>\n",
       "      <td>1.491485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8.525083</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>394.272727</td>\n",
       "      <td>5.522007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.795826</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>91</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>0.339908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars  Z_Score_Words  Paragraphs  No.break tags  Percentage_Upper_Case  \\\n",
       "0      3       6.453577           1              0                      3   \n",
       "1      5       1.394079           3              4                      3   \n",
       "2      4       3.666459           4              6                      4   \n",
       "3      4       8.525083          11             20                      3   \n",
       "4      5       1.795826           2              1                      6   \n",
       "\n",
       "   Percentage_Lower_Case  Avg_len_paragraph_per_review  Z_Score_HelpfulVotes  \n",
       "0                     93                   3087.000000             -0.235881  \n",
       "1                     91                    300.000000              0.915696  \n",
       "2                     90                    468.500000              1.491485  \n",
       "3                     91                    394.272727              5.522007  \n",
       "4                     91                    492.000000              0.339908  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'../FinalFeatures.csv')\n",
    "\n",
    "# The below line of code is to keep the z-scores of helpful votes and words and remove the actual values.\n",
    "\n",
    "print(dataset.columns)\n",
    "dataset = dataset[['Stars','Z_Score_Words', 'Paragraphs','No.break tags','Percentage_Upper_Case','Percentage_Lower_Case','Avg_len_paragraph_per_review','Z_Score_HelpfulVotes']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/WorkingCopy/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Z_Score_Words</th>\n",
       "      <th>Paragraphs</th>\n",
       "      <th>No.break tags</th>\n",
       "      <th>Percentage_Upper_Case</th>\n",
       "      <th>Percentage_Lower_Case</th>\n",
       "      <th>Avg_len_paragraph_per_review</th>\n",
       "      <th>Z_Score_HelpfulVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.315262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.417478</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.091377</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.040449</td>\n",
       "      <td>0.025593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.191931</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.063244</td>\n",
       "      <td>0.038147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.406928</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>0.134228</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.053202</td>\n",
       "      <td>0.126026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.109154</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.066423</td>\n",
       "      <td>0.013038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars  Z_Score_Words  Paragraphs  No.break tags  Percentage_Upper_Case  \\\n",
       "0   0.50       0.315262    0.000000       0.000000                   0.03   \n",
       "1   1.00       0.091377    0.015873       0.026846                   0.03   \n",
       "2   0.75       0.191931    0.023810       0.040268                   0.04   \n",
       "3   0.75       0.406928    0.079365       0.134228                   0.03   \n",
       "4   1.00       0.109154    0.007937       0.006711                   0.06   \n",
       "\n",
       "   Percentage_Lower_Case  Avg_len_paragraph_per_review  Z_Score_HelpfulVotes  \n",
       "0                   0.93                      0.417478              0.000484  \n",
       "1                   0.91                      0.040449              0.025593  \n",
       "2                   0.90                      0.063244              0.038147  \n",
       "3                   0.91                      0.053202              0.126026  \n",
       "4                   0.91                      0.066423              0.013038  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "dataset_new = scaler.fit_transform(dataset)\n",
    "dataset_new = pd.DataFrame(dataset_new,columns=dataset.columns)\n",
    "dataset_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the independant variables from the dependant variable which is \"Helpful Votes\" in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_new.iloc[:,0:-1].values\n",
    "y = dataset_new.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splitting the data into training data and testing data\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing keras and other required functions\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/WorkingCopy/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/envs/WorkingCopy/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 4s 182us/step - loss: 1.0318e-04\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 7.2799e-05\n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 5.8760e-05\n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 6.4967e-05\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 6.2808e-05\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 5.7783e-05\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 4.8736e-05\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 4.2406e-05\n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 4.6181e-05\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 3.9334e-05\n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 4.6757e-05\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 3.9322e-05\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 4s 179us/step - loss: 3.7641e-05\n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 3.7980e-05\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 4s 177us/step - loss: 3.1358e-05\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 5s 185us/step - loss: 3.2537e-05\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 4s 183us/step - loss: 3.3092e-05 0s - loss: 3.3142e-0\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 5s 187us/step - loss: 3.4599e-05\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 5s 186us/step - loss: 3.3090e-05\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 3.4375e-05\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 3.1538e-05\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 3.1395e-05\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 3.0081e-05\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 3.5984e-05\n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 4s 178us/step - loss: 2.9279e-05\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 2.2711e-05\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 3.4605e-05\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 2.8092e-05\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 2.5257e-05\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 2.9341e-05\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 3.0754e-05\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 2.3735e-05\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 2.9898e-05\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 2.2515e-05\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 5s 193us/step - loss: 2.6971e-05\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 2.1776e-05\n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 2.2964e-05\n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 2.5140e-05\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 2.6051e-05\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 2.5484e-05\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 1.7784e-05\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 1.6572e-05\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 2.9491e-05\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 2.5691e-05\n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 2.5540e-05\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 1.9538e-05\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 2.0999e-05\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 1.7930e-05\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 2.8780e-05\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 1.8773e-05\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 1.6557e-05\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 2.8347e-05\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 2.3533e-05\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 1.7026e-05\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 2.1844e-05\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 5s 190us/step - loss: 2.6731e-05\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 2.0473e-05\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 1.7715e-05\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 2.2220e-05\n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 1.5770e-05\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 1.5986e-05\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 1.5384e-05\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 4s 178us/step - loss: 1.6124e-05\n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 2.3065e-05\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 1.9603e-05\n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 1.8751e-05\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 4s 179us/step - loss: 2.7291e-05\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 1.6983e-05\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 1.7356e-05\n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 2.2665e-05\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 1.7025e-05\n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 2.4049e-05\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 1.9379e-05\n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 1.5492e-05\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 1.4869e-05\n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 1.8233e-05\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 1.8132e-05\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 1.3609e-05\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 1.0965e-05\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 1.4965e-05\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 2.5236e-05\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24362/24362 [==============================] - 4s 164us/step - loss: 1.5348e-05\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 1.3125e-05\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 1.4904e-05\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 1.8078e-05\n",
      "Epoch 86/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 2.1225e-05\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 2.2326e-05\n",
      "Epoch 88/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 2.3393e-05\n",
      "Epoch 89/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 1.5870e-05\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 1.9628e-05\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 1.3682e-05\n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 1.7004e-05\n",
      "Epoch 93/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 1.2799e-05\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 1.4010e-05\n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 2.4960e-05\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 1.6557e-05\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 1.4932e-05\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 1.5508e-05 0s - l\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 1.6117e-05\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 1.4537e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135efe860>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Building a 3 layer ANN with high number of nodes \"\"\"\n",
    "\n",
    "regressor = Sequential()\n",
    "regressor.add(Dense(100, kernel_initializer = 'normal',activation = 'relu',input_dim = 7))\n",
    "regressor.add(Dense(100, kernel_initializer = 'normal', activation = 'relu'))\n",
    "regressor.add(Dense(1, kernel_initializer = 'normal'))\n",
    "\n",
    "\"\"\" Compiling the regressor \"\"\"\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "\"\"\" Fitting the Artifilial Neural Network to our training data \"\"\"\n",
    "regressor.fit(X_train, y_train, batch_size=5, epochs=100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.012240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.004908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.031379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>-0.000439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.090217</td>\n",
       "      <td>0.078832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.054539</td>\n",
       "      <td>0.049852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.054539</td>\n",
       "      <td>0.052124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.023537</td>\n",
       "      <td>0.022205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.011101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>-0.000577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>0.023537</td>\n",
       "      <td>0.022244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>-0.000731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>-0.001443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>-0.000758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.009382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>-0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6055</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6058</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6059</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6060</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6061</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6063</th>\n",
       "      <td>0.357800</td>\n",
       "      <td>0.351061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6064</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6065</th>\n",
       "      <td>0.015691</td>\n",
       "      <td>0.012619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6071 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual Values  Predicted Values\n",
       "0          0.007846          0.012240\n",
       "1          0.001022          0.000790\n",
       "2          0.000000         -0.000830\n",
       "3          0.001349          0.004908\n",
       "4          0.001022          0.000808\n",
       "5          0.036700          0.031379\n",
       "6          0.001022          0.001554\n",
       "7          0.000484          0.000336\n",
       "8          0.001022         -0.000439\n",
       "9          0.000000         -0.000785\n",
       "10         0.001022          0.000749\n",
       "11         0.090217          0.078832\n",
       "12         0.001022          0.001428\n",
       "13         0.001022          0.001676\n",
       "14         0.000000         -0.000680\n",
       "15         0.001022          0.000212\n",
       "16         0.001022          0.000884\n",
       "17         0.054539          0.049852\n",
       "18         0.001022          0.001373\n",
       "19         0.001022          0.000829\n",
       "20         0.000000         -0.000820\n",
       "21         0.054539          0.052124\n",
       "22         0.000000         -0.001774\n",
       "23         0.001022          0.000514\n",
       "24         0.001022          0.001902\n",
       "25         0.001022          0.001253\n",
       "26         0.001022          0.002191\n",
       "27         0.023537          0.022205\n",
       "28         0.007846          0.011101\n",
       "29         0.000000         -0.000626\n",
       "...             ...               ...\n",
       "6041       0.001022         -0.000577\n",
       "6042       0.000000         -0.000931\n",
       "6043       0.023537          0.022244\n",
       "6044       0.000484         -0.000731\n",
       "6045       0.000000         -0.001022\n",
       "6046       0.001022          0.000490\n",
       "6047       0.001022          0.000761\n",
       "6048       0.000484         -0.001443\n",
       "6049       0.001022          0.001391\n",
       "6050       0.000000         -0.001543\n",
       "6051       0.001022         -0.000758\n",
       "6052       0.007846          0.009382\n",
       "6053       0.000484          0.000548\n",
       "6054       0.001022         -0.000058\n",
       "6055       0.001022          0.000299\n",
       "6056       0.000000          0.000243\n",
       "6057       0.001022          0.002507\n",
       "6058       0.000000         -0.000732\n",
       "6059       0.001022          0.000787\n",
       "6060       0.000000          0.000102\n",
       "6061       0.001022          0.001314\n",
       "6062       0.000000         -0.003071\n",
       "6063       0.357800          0.351061\n",
       "6064       0.001022          0.002692\n",
       "6065       0.015691          0.012619\n",
       "6066       0.000000         -0.001283\n",
       "6067       0.001022          0.001060\n",
       "6068       0.000000         -0.000787\n",
       "6069       0.000000         -0.000206\n",
       "6070       0.001022          0.001926\n",
       "\n",
       "[6071 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Predicting the values for Helpful Votes on the test data \"\"\"\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "\"\"\" Creating a dataframe to compare the actual values against predicted values \"\"\"\n",
    "y_pred = y_pred.reshape(6091,)\n",
    "temp = {'Actual Values': y_test,'Predicted Values': y_pred}\n",
    "y_compare = pd.DataFrame(temp)\n",
    "y_compare.head(-20)\n",
    "# \"\"\" Calculating the Mean Squared Error to estimate the efficiency of the ANN\"\"\"\n",
    "# y_compare['Mean Squared Error'] = ((np.diff(y_compare.values) ** 2).mean() ** .5)\n",
    "# y_compare.head(-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a good result when compared to the results obtained from scaling the data using StandartScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 4.4926e-04\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 4.4961e-04\n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 4.4976e-04\n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 4.5012e-04\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 4.4973e-04\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 4.4973e-04\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 4.4990e-04\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 4.4985e-04\n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 4s 180us/step - loss: 4.5046e-04\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 4.4999e-04\n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 4s 181us/step - loss: 4.4889e-04\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 4.4958e-04\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 4.5024e-04\n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 4.4992e-04\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 4.5087e-04\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 4.4975e-04\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 4.4988e-04\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 4.5036e-04\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 4.4932e-04\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 4.5025e-04\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 4.5007e-04\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 4.5011e-04\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 4.5067e-04\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 4.4955e-04\n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 4.5028e-04\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 4.4956e-04\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 4.4989e-04\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 4.4964e-04\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 4.5067e-04\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 4.4913e-04\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 4.4888e-04\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 4.4944e-04\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 4.5027e-04\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 4.4962e-04\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 4.4998e-04\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 4.4975e-04\n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 4.4978e-04\n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 4.4976e-04\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 4.4984e-04\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 4.4978e-04\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 4.5002e-04\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 4.4856e-04\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 4.4966e-04\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 4.4967e-04\n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 4.4914e-04\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 4.5032e-04\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 4.4917e-04\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 4.5011e-04\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 4.4975e-04\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 4.5043e-04\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 4.4910e-04\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 4.5035e-04\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 4.5043e-04\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 4.5055e-04\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 4.5011e-04\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 4.5015e-04\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 4.4962e-04\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 4.4966e-04\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 4.4973e-04\n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 4.5011e-04\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 4.4964e-04\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 4.4934e-04\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 4.4992e-04\n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 4.5039e-04\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 4.4954e-04 0s - loss: 4.5188e-\n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 4.5001e-04\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 4.5005e-04\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 4.5004e-04\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 4.5055e-04\n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 4.4985e-04\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 4.4932e-04\n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 4.5019e-04\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 4.5026e-04\n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 4.5003e-04\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 4.4982e-04\n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 4.5065e-04 2s -\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 4.5005e-04\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 4.4980e-04\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 4.5034e-04\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 4.4987e-04\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 4.5023e-04\n",
      "Epoch 82/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 4.5031e-04\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 4.5000e-04\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 4.5002e-04\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 4.5014e-04\n",
      "Epoch 86/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 4.4984e-04\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 4.5012e-04\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24362/24362 [==============================] - 4s 172us/step - loss: 4.4965e-04\n",
      "Epoch 89/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 4.4974e-04\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 4.5012e-04\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 4.4965e-04\n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 4.4966e-04\n",
      "Epoch 93/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 4.5012e-04\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 4.5013e-04\n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 4.5006e-04\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 4.4981e-04\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 4.4959e-04\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 4.5002e-04\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 4.5036e-04\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 4.5060e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13635c668>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Building a 3 layer ANN with less number of nodes \"\"\"\n",
    "\n",
    "regressor_1 = Sequential()\n",
    "regressor_1.add(Dense(4, kernel_initializer = 'normal',activation = 'relu',input_dim = 7))\n",
    "regressor_1.add(Dense(4, kernel_initializer = 'normal', activation = 'relu'))\n",
    "regressor_1.add(Dense(1, kernel_initializer = 'normal'))\n",
    "\n",
    "\"\"\" Compiling the regressor \"\"\"\n",
    "regressor_1.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "\"\"\" Fitting the Artifilial Neural Network to our training data \"\"\"\n",
    "regressor_1.fit(X_train, y_train, batch_size=5, epochs=100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.090217</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.054539</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.054539</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.023537</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>0.023537</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6055</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6058</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6059</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6060</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6061</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6063</th>\n",
       "      <td>0.357800</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6064</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6065</th>\n",
       "      <td>0.015691</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6071 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual Values  Predicted Values\n",
       "0          0.007846          0.004992\n",
       "1          0.001022          0.004992\n",
       "2          0.000000          0.004992\n",
       "3          0.001349          0.004992\n",
       "4          0.001022          0.004992\n",
       "5          0.036700          0.004992\n",
       "6          0.001022          0.004992\n",
       "7          0.000484          0.004992\n",
       "8          0.001022          0.004992\n",
       "9          0.000000          0.004992\n",
       "10         0.001022          0.004992\n",
       "11         0.090217          0.004992\n",
       "12         0.001022          0.004992\n",
       "13         0.001022          0.004992\n",
       "14         0.000000          0.004992\n",
       "15         0.001022          0.004992\n",
       "16         0.001022          0.004992\n",
       "17         0.054539          0.004992\n",
       "18         0.001022          0.004992\n",
       "19         0.001022          0.004992\n",
       "20         0.000000          0.004992\n",
       "21         0.054539          0.004992\n",
       "22         0.000000          0.004992\n",
       "23         0.001022          0.004992\n",
       "24         0.001022          0.004992\n",
       "25         0.001022          0.004992\n",
       "26         0.001022          0.004992\n",
       "27         0.023537          0.004992\n",
       "28         0.007846          0.004992\n",
       "29         0.000000          0.004992\n",
       "...             ...               ...\n",
       "6041       0.001022          0.004992\n",
       "6042       0.000000          0.004992\n",
       "6043       0.023537          0.004992\n",
       "6044       0.000484          0.004992\n",
       "6045       0.000000          0.004992\n",
       "6046       0.001022          0.004992\n",
       "6047       0.001022          0.004992\n",
       "6048       0.000484          0.004992\n",
       "6049       0.001022          0.004992\n",
       "6050       0.000000          0.004992\n",
       "6051       0.001022          0.004992\n",
       "6052       0.007846          0.004992\n",
       "6053       0.000484          0.004992\n",
       "6054       0.001022          0.004992\n",
       "6055       0.001022          0.004992\n",
       "6056       0.000000          0.004992\n",
       "6057       0.001022          0.004992\n",
       "6058       0.000000          0.004992\n",
       "6059       0.001022          0.004992\n",
       "6060       0.000000          0.004992\n",
       "6061       0.001022          0.004992\n",
       "6062       0.000000          0.004992\n",
       "6063       0.357800          0.004992\n",
       "6064       0.001022          0.004992\n",
       "6065       0.015691          0.004992\n",
       "6066       0.000000          0.004992\n",
       "6067       0.001022          0.004992\n",
       "6068       0.000000          0.004992\n",
       "6069       0.000000          0.004992\n",
       "6070       0.001022          0.004992\n",
       "\n",
       "[6071 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Predicting the values for Helpful Votes on the test data \"\"\"\n",
    "y_pred_1 = regressor_1.predict(X_test)\n",
    "\n",
    "\"\"\" Creating a dataframe to compare the actual values against predicted values \"\"\"\n",
    "y_pred_1 = y_pred_1.reshape(6091,)\n",
    "temp_1 = {'Actual Values': y_test,'Predicted Values': y_pred_1}\n",
    "y_compare_1 = pd.DataFrame(temp_1)\n",
    "\n",
    "# \"\"\" Calculating the Mean Squared Error to estimate the efficiency of the ANN\"\"\"\n",
    "# y_compare_1['Mean Squared Error'] = ((np.diff(y_compare_1.values) ** 2).mean() ** .5)\n",
    "y_compare_1.head(-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 7s 301us/step - loss: 4.4999e-04\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 7s 275us/step - loss: 3.4834e-04\n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 7s 269us/step - loss: 1.1624e-04\n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 7s 275us/step - loss: 4.3529e-04\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 7s 267us/step - loss: 9.8979e-05\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 6s 263us/step - loss: 1.0418e-04\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 6s 263us/step - loss: 1.8850e-04\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 6s 267us/step - loss: 1.0185e-04\n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 6s 265us/step - loss: 8.6297e-05\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 7s 272us/step - loss: 9.0458e-05\n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 7s 279us/step - loss: 8.8997e-04\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 7s 283us/step - loss: 9.2808e-05\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 7s 273us/step - loss: 8.1385e-05\n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 7s 276us/step - loss: 6.5217e-05\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 7s 271us/step - loss: 3.8786e-04\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 6s 261us/step - loss: 5.8456e-05\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 6s 267us/step - loss: 8.5641e-05\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 7s 274us/step - loss: 6.3524e-05\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 7s 275us/step - loss: 7.4097e-05\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 7s 274us/step - loss: 7.7269e-05\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 7s 272us/step - loss: 6.4210e-05\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 6s 262us/step - loss: 1.7701e-04\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 6s 259us/step - loss: 5.5654e-05\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 6s 257us/step - loss: 7.3330e-05\n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 6s 266us/step - loss: 7.4898e-05\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 6s 265us/step - loss: 6.5076e-05\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 6s 257us/step - loss: 2.7094e-04\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 6s 266us/step - loss: 8.8920e-05\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 6s 267us/step - loss: 5.6268e-05\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 6s 256us/step - loss: 2.0063e-04\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 6s 254us/step - loss: 4.5006e-05\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 6s 257us/step - loss: 5.0211e-05\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 6s 267us/step - loss: 4.6506e-05\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 7s 270us/step - loss: 5.3786e-05\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 6s 262us/step - loss: 8.5843e-05\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 5.3119e-05\n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 6s 265us/step - loss: 1.7848e-04\n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 5.7737e-05\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 7s 267us/step - loss: 1.5861e-04\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 6s 265us/step - loss: 7.0105e-05\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 6s 263us/step - loss: 5.5808e-05\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 6s 263us/step - loss: 4.1261e-05\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 7s 268us/step - loss: 9.7079e-05\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 7s 268us/step - loss: 5.4724e-05\n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 6s 267us/step - loss: 6.2238e-05\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 7s 278us/step - loss: 4.1270e-05\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 7s 273us/step - loss: 4.2168e-05\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 6s 266us/step - loss: 4.5550e-05\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 7s 275us/step - loss: 3.3496e-05\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 3.7143e-05\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 3.8533e-05\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 4.9545e-05\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 3.8533e-05\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 7s 274us/step - loss: 5.2699e-05\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 7s 272us/step - loss: 3.5263e-05\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 7s 270us/step - loss: 4.2453e-05\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 7s 267us/step - loss: 2.6674e-05\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 7s 267us/step - loss: 7.1262e-05\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 6s 265us/step - loss: 2.9715e-05\n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 6s 263us/step - loss: 3.3132e-05\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 6s 265us/step - loss: 6.6341e-05\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 3.8775e-05\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 3.2954e-05\n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 6s 263us/step - loss: 7.8737e-05\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 6s 263us/step - loss: 3.8387e-05\n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 6s 265us/step - loss: 1.0374e-04\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 3.4388e-05\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 6s 263us/step - loss: 2.8652e-05\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 7s 269us/step - loss: 4.2362e-05\n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 2.3904e-05\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 2.8178e-05\n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 7s 267us/step - loss: 3.0395e-05\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 0.0031\n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 6s 266us/step - loss: 4.8827e-05\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 6s 267us/step - loss: 3.8119e-05\n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 7s 267us/step - loss: 3.1473e-05\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 7s 267us/step - loss: 8.7200e-05\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 6s 263us/step - loss: 6.8928e-05\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 3.9260e-05\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 7s 269us/step - loss: 3.3329e-05\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 7s 270us/step - loss: 2.9967e-05\n",
      "Epoch 82/100\n",
      "24362/24362 [==============================] - 6s 263us/step - loss: 4.6132e-05\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 3.0299e-05\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 7s 274us/step - loss: 4.0264e-04\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 6s 266us/step - loss: 3.0785e-05\n",
      "Epoch 86/100\n",
      "24362/24362 [==============================] - 7s 267us/step - loss: 2.1237e-05\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 6s 267us/step - loss: 3.0760e-05\n",
      "Epoch 88/100\n",
      "24362/24362 [==============================] - 6s 267us/step - loss: 2.8919e-05\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24362/24362 [==============================] - 7s 289us/step - loss: 2.5099e-05\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 6s 265us/step - loss: 2.5999e-05\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 6s 262us/step - loss: 3.0614e-05\n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 6s 256us/step - loss: 2.7197e-05\n",
      "Epoch 93/100\n",
      "24362/24362 [==============================] - 6s 257us/step - loss: 3.3447e-05\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 6s 256us/step - loss: 2.2715e-05\n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 6s 256us/step - loss: 2.2191e-05\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 6s 260us/step - loss: 1.9766e-05\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 6s 265us/step - loss: 2.1680e-05\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 6s 266us/step - loss: 2.2042e-05\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 6s 262us/step - loss: 2.2795e-05\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 6s 261us/step - loss: 3.1510e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1367e9b00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Building a deep and wide ANN with high number of nodes \"\"\"\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "regressor_2 = Sequential()\n",
    "regressor_2.add(Dense(50, kernel_initializer = 'normal',input_dim = 7))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(100, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(200, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(100, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(50, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(20, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(1, kernel_initializer = 'normal'))\n",
    "\n",
    "\"\"\" Compiling the regressor \"\"\"\n",
    "regressor_2.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "\"\"\" Fitting the Artifilial Neural Network to our training data \"\"\"\n",
    "regressor_2.fit(X_train, y_train, batch_size=5, epochs=100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.011333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.003060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.007321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.034202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.001222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.090217</td>\n",
       "      <td>0.060896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.003052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.054539</td>\n",
       "      <td>0.057727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.003182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.054539</td>\n",
       "      <td>0.062347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.023537</td>\n",
       "      <td>0.022042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.010519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>0.023537</td>\n",
       "      <td>0.022417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.003466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.003519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.001680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.008152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.001098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6055</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6058</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6059</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6060</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6061</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6063</th>\n",
       "      <td>0.357800</td>\n",
       "      <td>0.318801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6064</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6065</th>\n",
       "      <td>0.015691</td>\n",
       "      <td>0.015878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.000867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6071 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual Values  Predicted Values\n",
       "0          0.007846          0.011333\n",
       "1          0.001022          0.003060\n",
       "2          0.000000          0.000617\n",
       "3          0.001349          0.007321\n",
       "4          0.001022          0.001133\n",
       "5          0.036700          0.034202\n",
       "6          0.001022          0.002789\n",
       "7          0.000484          0.001222\n",
       "8          0.001022          0.001471\n",
       "9          0.000000          0.000599\n",
       "10         0.001022          0.001238\n",
       "11         0.090217          0.060896\n",
       "12         0.001022          0.000976\n",
       "13         0.001022          0.001826\n",
       "14         0.000000          0.000418\n",
       "15         0.001022          0.003052\n",
       "16         0.001022          0.001336\n",
       "17         0.054539          0.057727\n",
       "18         0.001022          0.003182\n",
       "19         0.001022          0.002492\n",
       "20         0.000000          0.001543\n",
       "21         0.054539          0.062347\n",
       "22         0.000000          0.000701\n",
       "23         0.001022          0.002145\n",
       "24         0.001022          0.002680\n",
       "25         0.001022          0.001774\n",
       "26         0.001022          0.002073\n",
       "27         0.023537          0.022042\n",
       "28         0.007846          0.010519\n",
       "29         0.000000          0.000491\n",
       "...             ...               ...\n",
       "6041       0.001022          0.002278\n",
       "6042       0.000000          0.000270\n",
       "6043       0.023537          0.022417\n",
       "6044       0.000484          0.000837\n",
       "6045       0.000000          0.000421\n",
       "6046       0.001022          0.003466\n",
       "6047       0.001022          0.003519\n",
       "6048       0.000484          0.001680\n",
       "6049       0.001022          0.002895\n",
       "6050       0.000000          0.000560\n",
       "6051       0.001022          0.001335\n",
       "6052       0.007846          0.008152\n",
       "6053       0.000484          0.001098\n",
       "6054       0.001022          0.001033\n",
       "6055       0.001022          0.002819\n",
       "6056       0.000000          0.002511\n",
       "6057       0.001022          0.002371\n",
       "6058       0.000000         -0.000796\n",
       "6059       0.001022          0.002432\n",
       "6060       0.000000          0.000789\n",
       "6061       0.001022          0.001502\n",
       "6062       0.000000          0.000807\n",
       "6063       0.357800          0.318801\n",
       "6064       0.001022          0.001920\n",
       "6065       0.015691          0.015878\n",
       "6066       0.000000          0.000712\n",
       "6067       0.001022          0.001373\n",
       "6068       0.000000          0.000525\n",
       "6069       0.000000          0.000797\n",
       "6070       0.001022          0.000867\n",
       "\n",
       "[6071 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Predicting the values for Helpful Votes on the test data \"\"\"\n",
    "y_pred_2 = regressor_2.predict(X_test)\n",
    "\n",
    "\"\"\" Creating a dataframe to compare the actual values against predicted values \"\"\"\n",
    "y_pred_2 = y_pred_2.reshape(6091,)\n",
    "temp_2 = {'Actual Values': y_test,'Predicted Values': y_pred_2}\n",
    "y_compare_2 = pd.DataFrame(temp_2)\n",
    "\n",
    "# \"\"\" Calculating the Mean Squared Error to estimate the efficiency of the ANN\"\"\"\n",
    "# y_compare_2['Mean Squared Error'] = ((np.diff(y_compare_1.values) ** 2).mean() ** .5)\n",
    "y_compare_2.head(-20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
