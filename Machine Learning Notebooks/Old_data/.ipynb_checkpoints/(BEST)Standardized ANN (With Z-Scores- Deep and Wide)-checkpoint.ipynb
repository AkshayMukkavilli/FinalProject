{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use the same Artificial Neural Networks as in ANN(Deep and Wide with High Nodes Vs Low Nodes) but we will scale the dataset using StandardScaler from sklearn and also use the dataset with Z_Scores of Words and Helpful Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Stars', 'Helpful Votes', 'Z_Score_HelpfulVotes', 'Words',\n",
      "       'Z_Score_Words', 'Paragraphs', 'No.break tags', 'Percentage_Upper_Case',\n",
      "       'Percentage_Lower_Case', 'Avg_len_paragraph_per_review'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Z_Score_Words</th>\n",
       "      <th>Paragraphs</th>\n",
       "      <th>No.break tags</th>\n",
       "      <th>Percentage_Upper_Case</th>\n",
       "      <th>Percentage_Lower_Case</th>\n",
       "      <th>Avg_len_paragraph_per_review</th>\n",
       "      <th>Z_Score_HelpfulVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>6.453577</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>3087.000000</td>\n",
       "      <td>-0.235881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1.394079</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>0.915696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3.666459</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>90</td>\n",
       "      <td>468.500000</td>\n",
       "      <td>1.491485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8.525083</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>394.272727</td>\n",
       "      <td>5.522007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.795826</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>91</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>0.339908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars  Z_Score_Words  Paragraphs  No.break tags  Percentage_Upper_Case  \\\n",
       "0      3       6.453577           1              0                      3   \n",
       "1      5       1.394079           3              4                      3   \n",
       "2      4       3.666459           4              6                      4   \n",
       "3      4       8.525083          11             20                      3   \n",
       "4      5       1.795826           2              1                      6   \n",
       "\n",
       "   Percentage_Lower_Case  Avg_len_paragraph_per_review  Z_Score_HelpfulVotes  \n",
       "0                     93                   3087.000000             -0.235881  \n",
       "1                     91                    300.000000              0.915696  \n",
       "2                     90                    468.500000              1.491485  \n",
       "3                     91                    394.272727              5.522007  \n",
       "4                     91                    492.000000              0.339908  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'../FinalFeatures.csv')\n",
    "\n",
    "# The below line of code is to keep the z-scores of helpful votes and words and remove the actual values.\n",
    "\n",
    "print(dataset.columns)\n",
    "dataset = dataset[['Stars','Z_Score_Words', 'Paragraphs','No.break tags','Percentage_Upper_Case','Percentage_Lower_Case','Avg_len_paragraph_per_review','Z_Score_HelpfulVotes']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the independant variables from the dependant variable which is \"Helpful Votes\" in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,0:-1].values\n",
    "y = dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splitting the data into training data and testing data\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2,random_state=0)\n",
    "\n",
    "\"\"\"Scaling the data using StandardScaler from sklearn package\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing keras and other required functions\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/WorkingCopy/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/envs/WorkingCopy/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 5s 197us/step - loss: 0.1338\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 5s 187us/step - loss: 0.1340\n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 5s 186us/step - loss: 0.0893\n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 5s 216us/step - loss: 0.0984\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 0.0893\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 0.0734\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 4s 178us/step - loss: 0.1048\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0714\n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0709\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 0.0554\n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 0.0758\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 5s 189us/step - loss: 0.0778\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0623\n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0989\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0457\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0545\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0458\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0497\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 0.0570\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0449\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 0.0435\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0375\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0732\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0332\n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0433\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0443\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 0.0492\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.1249\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0447\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0300\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0484\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0388\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0365\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0395\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0282\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.1266\n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0274\n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0416\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0248\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0896\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0159\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0317\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.1190\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 0.0255\n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0139\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0396\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0497\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 0.0315\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0331\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0270\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0197\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0451\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0146\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0242\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0507\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 0.0342\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0488\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0249\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0257\n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 0.0133\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0193\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0306\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0412\n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 0.0160\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0249\n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0179\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0176\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0950\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0078\n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0206\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 0.0571\n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 0.0279\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0131 \n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0208\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 0.0443\n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 0.0189\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0241\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 5s 192us/step - loss: 0.0424\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 0.0125\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 4s 181us/step - loss: 0.0392\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 5s 187us/step - loss: 0.0254 0s - loss: \n",
      "Epoch 82/100\n",
      "24362/24362 [==============================] - 4s 181us/step - loss: 0.0194\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 0.0304\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 4s 179us/step - loss: 0.0136\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 5s 185us/step - loss: 0.0201\n",
      "Epoch 86/100\n",
      "24362/24362 [==============================] - 4s 177us/step - loss: 0.0137\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 5s 194us/step - loss: 0.0190\n",
      "Epoch 88/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 0.0134\n",
      "Epoch 89/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0304\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 0.0210\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 0.0176 1s - loss: - ETA: 1s - loss:  - ETA: 0s - loss: 0. - ETA: 0s \n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 0.0241\n",
      "Epoch 93/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 0.0271\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 0.0264\n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0546\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 0.0185\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0210\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0128\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 0.0473\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x136a62240>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Building a 3 layer ANN with high number of nodes \"\"\"\n",
    "\n",
    "regressor = Sequential()\n",
    "regressor.add(Dense(100, kernel_initializer = 'normal',activation = 'relu',input_dim = 7))\n",
    "regressor.add(Dense(100, kernel_initializer = 'normal', activation = 'relu'))\n",
    "regressor.add(Dense(1, kernel_initializer = 'normal'))\n",
    "\n",
    "\"\"\" Compiling the regressor \"\"\"\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "\"\"\" Fitting the Artifilial Neural Network to our training data \"\"\"\n",
    "regressor.fit(X_train, y_train, batch_size=5, epochs=100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.101746</td>\n",
       "      <td>0.404272</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.217275</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.253881</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.196210</td>\n",
       "      <td>-0.166533</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.211747</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.425145</td>\n",
       "      <td>1.241110</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.218867</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.235881</td>\n",
       "      <td>-0.221329</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.235977</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.256196</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.209296</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.879658</td>\n",
       "      <td>2.362285</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.201441</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.192444</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.249608</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.223832</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.213989</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.243316</td>\n",
       "      <td>1.758164</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.202548</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.217150</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.255655</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.243316</td>\n",
       "      <td>1.897513</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.251847</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.225858</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.201937</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.222215</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.222457</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.821420</td>\n",
       "      <td>0.686338</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.101746</td>\n",
       "      <td>0.084638</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.248135</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.210462</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.263199</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>0.821420</td>\n",
       "      <td>0.688128</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>-0.235881</td>\n",
       "      <td>-0.233072</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.244414</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.220435</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.214319</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>-0.235881</td>\n",
       "      <td>-0.236183</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.192367</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.253920</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.215831</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0.101746</td>\n",
       "      <td>0.128612</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>-0.235881</td>\n",
       "      <td>-0.214819</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.222111</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6055</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.209226</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.222138</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.195872</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6058</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.248115</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6059</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.207184</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6060</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.238365</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6061</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.201593</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.220044</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6063</th>\n",
       "      <td>16.152226</td>\n",
       "      <td>15.285624</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6064</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.187445</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6065</th>\n",
       "      <td>0.461583</td>\n",
       "      <td>0.367572</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.254899</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.203237</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.238896</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.248012</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.184341</td>\n",
       "      <td>0.146315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6071 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual Values  Predicted Values  Mean Squared Error\n",
       "0          0.101746          0.404272            0.146315\n",
       "1         -0.211198         -0.217275            0.146315\n",
       "2         -0.258092         -0.253881            0.146315\n",
       "3         -0.196210         -0.166533            0.146315\n",
       "4         -0.211198         -0.211747            0.146315\n",
       "5          1.425145          1.241110            0.146315\n",
       "6         -0.211198         -0.218867            0.146315\n",
       "7         -0.235881         -0.221329            0.146315\n",
       "8         -0.211198         -0.235977            0.146315\n",
       "9         -0.258092         -0.256196            0.146315\n",
       "10        -0.211198         -0.209296            0.146315\n",
       "11         3.879658          2.362285            0.146315\n",
       "12        -0.211198         -0.201441            0.146315\n",
       "13        -0.211198         -0.192444            0.146315\n",
       "14        -0.258092         -0.249608            0.146315\n",
       "15        -0.211198         -0.223832            0.146315\n",
       "16        -0.211198         -0.213989            0.146315\n",
       "17         2.243316          1.758164            0.146315\n",
       "18        -0.211198         -0.202548            0.146315\n",
       "19        -0.211198         -0.217150            0.146315\n",
       "20        -0.258092         -0.255655            0.146315\n",
       "21         2.243316          1.897513            0.146315\n",
       "22        -0.258092         -0.251847            0.146315\n",
       "23        -0.211198         -0.225858            0.146315\n",
       "24        -0.211198         -0.201937            0.146315\n",
       "25        -0.211198         -0.222215            0.146315\n",
       "26        -0.211198         -0.222457            0.146315\n",
       "27         0.821420          0.686338            0.146315\n",
       "28         0.101746          0.084638            0.146315\n",
       "29        -0.258092         -0.248135            0.146315\n",
       "...             ...               ...                 ...\n",
       "6041      -0.211198         -0.210462            0.146315\n",
       "6042      -0.258092         -0.263199            0.146315\n",
       "6043       0.821420          0.688128            0.146315\n",
       "6044      -0.235881         -0.233072            0.146315\n",
       "6045      -0.258092         -0.244414            0.146315\n",
       "6046      -0.211198         -0.220435            0.146315\n",
       "6047      -0.211198         -0.214319            0.146315\n",
       "6048      -0.235881         -0.236183            0.146315\n",
       "6049      -0.211198         -0.192367            0.146315\n",
       "6050      -0.258092         -0.253920            0.146315\n",
       "6051      -0.211198         -0.215831            0.146315\n",
       "6052       0.101746          0.128612            0.146315\n",
       "6053      -0.235881         -0.214819            0.146315\n",
       "6054      -0.211198         -0.222111            0.146315\n",
       "6055      -0.211198         -0.209226            0.146315\n",
       "6056      -0.258092         -0.222138            0.146315\n",
       "6057      -0.211198         -0.195872            0.146315\n",
       "6058      -0.258092         -0.248115            0.146315\n",
       "6059      -0.211198         -0.207184            0.146315\n",
       "6060      -0.258092         -0.238365            0.146315\n",
       "6061      -0.211198         -0.201593            0.146315\n",
       "6062      -0.258092         -0.220044            0.146315\n",
       "6063      16.152226         15.285624            0.146315\n",
       "6064      -0.211198         -0.187445            0.146315\n",
       "6065       0.461583          0.367572            0.146315\n",
       "6066      -0.258092         -0.254899            0.146315\n",
       "6067      -0.211198         -0.203237            0.146315\n",
       "6068      -0.258092         -0.238896            0.146315\n",
       "6069      -0.258092         -0.248012            0.146315\n",
       "6070      -0.211198         -0.184341            0.146315\n",
       "\n",
       "[6071 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Predicting the values for Helpful Votes on the test data \"\"\"\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "\"\"\" Creating a dataframe to compare the actual values against predicted values \"\"\"\n",
    "y_pred = y_pred.reshape(6091,)\n",
    "temp = {'Actual Values': y_test,'Predicted Values': y_pred}\n",
    "y_compare = pd.DataFrame(temp)\n",
    "\n",
    "\"\"\" Calculating the Mean Squared Error to estimate the efficiency of the ANN\"\"\"\n",
    "y_compare['Mean Squared Error'] = ((np.diff(y_compare.values) ** 2).mean() ** .5)\n",
    "y_compare.head(-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best result that we have obtained so far. This ANN might potentially be the solution for our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 4s 178us/step - loss: 0.2520\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 0.1106\n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 0.0982\n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 0.0967\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 0.0935\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 0.0881\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0874\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 0.0806\n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 0.0810\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0717\n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 0.0712\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 0.0695\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0674\n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 0.0692\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 0.0682\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0622\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0651\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0660\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0622\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 0.0637\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 0.0608\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0542\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0576\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 0.0583\n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 5s 190us/step - loss: 0.0590\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0578\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0530\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 4s 177us/step - loss: 0.0566\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 0.0605\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 0.0507\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 0.0588\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 0.0553\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 0.0598\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 0.0592\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 5s 186us/step - loss: 0.0541\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 4s 184us/step - loss: 0.0579\n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 4s 182us/step - loss: 0.0555\n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 0.0590\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 4s 178us/step - loss: 0.0573\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 4s 180us/step - loss: 0.0529\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 4s 180us/step - loss: 0.0534\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0585\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 0.0593\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0563\n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0586\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 0.0520\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 0.0519\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 0.0592\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 0.0550\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 0.0538\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0539\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 0.0482\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0497\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 4s 180us/step - loss: 0.0467\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0507\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 0.0520\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0473\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 0.0456\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0509\n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0458\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0514\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0471\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0507\n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 0.0477\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 0.0494\n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 0.0474\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0453\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0443\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0504\n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0512\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0499\n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 0.0476\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 0.0495\n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 0.0499\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 0.0482\n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0503\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0509\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0469\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 0.0439\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 0.0490\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 0.0462\n",
      "Epoch 82/100\n",
      "24362/24362 [==============================] - 4s 181us/step - loss: 0.0402\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 0.0496\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 0.0481\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 0.0505\n",
      "Epoch 86/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0493\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0514\n",
      "Epoch 88/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0492\n",
      "Epoch 89/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0459\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 0.0459\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 4s 178us/step - loss: 0.0431\n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 0.0488\n",
      "Epoch 93/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 0.0520\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 4s 177us/step - loss: 0.0437\n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 0.0483\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 0.0496\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 0.0435\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 0.0495\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 0.0535\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 0.0476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x136ebd160>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Building a 3 layer ANN with less number of nodes \"\"\"\n",
    "\n",
    "regressor_1 = Sequential()\n",
    "regressor_1.add(Dense(4, kernel_initializer = 'normal',activation = 'relu',input_dim = 7))\n",
    "regressor_1.add(Dense(4, kernel_initializer = 'normal', activation = 'relu'))\n",
    "regressor_1.add(Dense(1, kernel_initializer = 'normal'))\n",
    "\n",
    "\"\"\" Compiling the regressor \"\"\"\n",
    "regressor_1.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "\"\"\" Fitting the Artifilial Neural Network to our training data \"\"\"\n",
    "regressor_1.fit(X_train, y_train, batch_size=5, epochs=100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.101746</td>\n",
       "      <td>-0.016128</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.196210</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.425145</td>\n",
       "      <td>1.178389</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.235881</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.879658</td>\n",
       "      <td>2.160279</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.226668</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.243316</td>\n",
       "      <td>1.628688</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.147974</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.213787</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.300219</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.243316</td>\n",
       "      <td>1.669706</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.195172</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.821420</td>\n",
       "      <td>0.510852</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.101746</td>\n",
       "      <td>-0.037189</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>0.821420</td>\n",
       "      <td>0.707214</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>-0.235881</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>-0.235881</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.197358</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0.101746</td>\n",
       "      <td>-0.114569</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>-0.235881</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6055</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.159913</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.217833</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.126830</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6058</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.241500</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6059</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6060</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6061</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.194799</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6063</th>\n",
       "      <td>16.152226</td>\n",
       "      <td>15.024093</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6064</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.126270</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6065</th>\n",
       "      <td>0.461583</td>\n",
       "      <td>0.392252</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.231478</td>\n",
       "      <td>0.215933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6071 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual Values  Predicted Values  Mean Squared Error\n",
       "0          0.101746         -0.016128            0.215933\n",
       "1         -0.211198         -0.231478            0.215933\n",
       "2         -0.258092         -0.231478            0.215933\n",
       "3         -0.196210          0.100586            0.215933\n",
       "4         -0.211198         -0.231478            0.215933\n",
       "5          1.425145          1.178389            0.215933\n",
       "6         -0.211198         -0.231478            0.215933\n",
       "7         -0.235881         -0.231478            0.215933\n",
       "8         -0.211198         -0.231478            0.215933\n",
       "9         -0.258092         -0.231478            0.215933\n",
       "10        -0.211198         -0.231478            0.215933\n",
       "11         3.879658          2.160279            0.215933\n",
       "12        -0.211198         -0.231478            0.215933\n",
       "13        -0.211198         -0.226668            0.215933\n",
       "14        -0.258092         -0.231478            0.215933\n",
       "15        -0.211198         -0.231478            0.215933\n",
       "16        -0.211198         -0.231478            0.215933\n",
       "17         2.243316          1.628688            0.215933\n",
       "18        -0.211198         -0.147974            0.215933\n",
       "19        -0.211198         -0.213787            0.215933\n",
       "20        -0.258092         -0.300219            0.215933\n",
       "21         2.243316          1.669706            0.215933\n",
       "22        -0.258092         -0.231478            0.215933\n",
       "23        -0.211198         -0.231478            0.215933\n",
       "24        -0.211198         -0.195172            0.215933\n",
       "25        -0.211198         -0.231478            0.215933\n",
       "26        -0.211198         -0.231478            0.215933\n",
       "27         0.821420          0.510852            0.215933\n",
       "28         0.101746         -0.037189            0.215933\n",
       "29        -0.258092         -0.231478            0.215933\n",
       "...             ...               ...                 ...\n",
       "6041      -0.211198         -0.231478            0.215933\n",
       "6042      -0.258092         -0.231478            0.215933\n",
       "6043       0.821420          0.707214            0.215933\n",
       "6044      -0.235881         -0.231478            0.215933\n",
       "6045      -0.258092         -0.231478            0.215933\n",
       "6046      -0.211198         -0.231478            0.215933\n",
       "6047      -0.211198         -0.231478            0.215933\n",
       "6048      -0.235881         -0.231478            0.215933\n",
       "6049      -0.211198         -0.197358            0.215933\n",
       "6050      -0.258092         -0.231478            0.215933\n",
       "6051      -0.211198         -0.231478            0.215933\n",
       "6052       0.101746         -0.114569            0.215933\n",
       "6053      -0.235881         -0.231478            0.215933\n",
       "6054      -0.211198         -0.231478            0.215933\n",
       "6055      -0.211198         -0.159913            0.215933\n",
       "6056      -0.258092         -0.217833            0.215933\n",
       "6057      -0.211198         -0.126830            0.215933\n",
       "6058      -0.258092         -0.241500            0.215933\n",
       "6059      -0.211198         -0.231478            0.215933\n",
       "6060      -0.258092         -0.231478            0.215933\n",
       "6061      -0.211198         -0.194799            0.215933\n",
       "6062      -0.258092         -0.231478            0.215933\n",
       "6063      16.152226         15.024093            0.215933\n",
       "6064      -0.211198         -0.126270            0.215933\n",
       "6065       0.461583          0.392252            0.215933\n",
       "6066      -0.258092         -0.231478            0.215933\n",
       "6067      -0.211198         -0.231478            0.215933\n",
       "6068      -0.258092         -0.231478            0.215933\n",
       "6069      -0.258092         -0.231478            0.215933\n",
       "6070      -0.211198         -0.231478            0.215933\n",
       "\n",
       "[6071 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Predicting the values for Helpful Votes on the test data \"\"\"\n",
    "y_pred_1 = regressor_1.predict(X_test)\n",
    "\n",
    "\"\"\" Creating a dataframe to compare the actual values against predicted values \"\"\"\n",
    "y_pred_1 = y_pred_1.reshape(6091,)\n",
    "temp_1 = {'Actual Values': y_test,'Predicted Values': y_pred_1}\n",
    "y_compare_1 = pd.DataFrame(temp_1)\n",
    "\n",
    "\"\"\" Calculating the Mean Squared Error to estimate the efficiency of the ANN\"\"\"\n",
    "y_compare_1['Mean Squared Error'] = ((np.diff(y_compare_1.values) ** 2).mean() ** .5)\n",
    "y_compare_1.head(-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ANN is the second best. The first one in this notebook is the best so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 7s 276us/step - loss: 0.1842\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 6s 258us/step - loss: 0.2703\n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 6s 265us/step - loss: 0.1626\n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 7s 268us/step - loss: 0.1733\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 6s 257us/step - loss: 0.2064\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 7s 268us/step - loss: 0.2890\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 7s 268us/step - loss: 0.2397\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 7s 277us/step - loss: 0.2253\n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 7s 289us/step - loss: 0.2949\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 7s 284us/step - loss: 0.1517\n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 7s 276us/step - loss: 0.2073\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 7s 276us/step - loss: 0.1240\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 7s 293us/step - loss: 0.2365\n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 7s 288us/step - loss: 0.2133\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 7s 287us/step - loss: 0.1687\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 7s 287us/step - loss: 0.2068\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 6s 260us/step - loss: 0.1277\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 8s 313us/step - loss: 0.2057\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 7s 275us/step - loss: 0.1774\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 7s 280us/step - loss: 0.1499\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 7s 291us/step - loss: 0.1625\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 7s 278us/step - loss: 0.6782\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 7s 278us/step - loss: 0.0785\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 7s 300us/step - loss: 0.1477\n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 7s 279us/step - loss: 0.1429\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 7s 275us/step - loss: 0.1438\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 6s 265us/step - loss: 0.2981\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 7s 277us/step - loss: 0.1323\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 6s 258us/step - loss: 0.2354\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 7s 268us/step - loss: 0.2315\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 7s 283us/step - loss: 0.1298\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 7s 270us/step - loss: 0.1612\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 6s 257us/step - loss: 0.1314\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 0.1898\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 6s 252us/step - loss: 3.5470\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 6s 257us/step - loss: 0.1038\n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 6s 261us/step - loss: 0.1926\n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 6s 255us/step - loss: 0.2646\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 6s 262us/step - loss: 0.1130\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 6s 257us/step - loss: 0.1431\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 6s 254us/step - loss: 0.7473\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 6s 262us/step - loss: 0.0951\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 6s 254us/step - loss: 0.1048\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 6s 261us/step - loss: 0.1142\n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 6s 258us/step - loss: 0.0879\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 7s 283us/step - loss: 0.1876\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 7s 281us/step - loss: 0.1633\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 7s 284us/step - loss: 0.1170\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 7s 288us/step - loss: 0.2763\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 7s 288us/step - loss: 0.2127\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 7s 274us/step - loss: 0.1167\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 7s 279us/step - loss: 0.2113\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 8s 309us/step - loss: 0.1732\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 8s 314us/step - loss: 0.2184\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 7s 277us/step - loss: 0.1387\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 8s 322us/step - loss: 0.1069\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 8s 309us/step - loss: 0.2212\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 7s 282us/step - loss: 0.1458\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 7s 286us/step - loss: 0.1344\n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 7s 292us/step - loss: 0.0842\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 7s 272us/step - loss: 0.2225\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 7s 270us/step - loss: 0.1610\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 7s 294us/step - loss: 0.1584 0s \n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 7s 293us/step - loss: 0.1558\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 7s 301us/step - loss: 0.2650\n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 7s 281us/step - loss: 0.3523\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 8s 313us/step - loss: 0.1689\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 7s 282us/step - loss: 0.1668\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 7s 271us/step - loss: 0.4111 \n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 7s 286us/step - loss: 0.2163\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 7s 276us/step - loss: 0.1677\n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 7s 279us/step - loss: 0.2596\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 7s 278us/step - loss: 0.1248\n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 7s 271us/step - loss: 0.1658\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 6s 261us/step - loss: 0.1886\n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 6s 261us/step - loss: 0.2434\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 6s 261us/step - loss: 0.1180\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 6s 254us/step - loss: 0.1516\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 6s 262us/step - loss: 0.1372\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 6s 261us/step - loss: 0.1958\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 7s 288us/step - loss: 0.2351\n",
      "Epoch 82/100\n",
      "24362/24362 [==============================] - 7s 283us/step - loss: 1.4039\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 7s 282us/step - loss: 0.1719\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 7s 268us/step - loss: 0.1312\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 7s 304us/step - loss: 0.1954\n",
      "Epoch 86/100\n",
      "24362/24362 [==============================] - 7s 286us/step - loss: 0.1241\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 7s 297us/step - loss: 0.1339\n",
      "Epoch 88/100\n",
      "24362/24362 [==============================] - 7s 285us/step - loss: 1.2755\n",
      "Epoch 89/100\n",
      "24362/24362 [==============================] - 7s 271us/step - loss: 0.1190\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 6s 263us/step - loss: 0.0953\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 7s 275us/step - loss: 0.1225\n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 7s 268us/step - loss: 0.1557\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24362/24362 [==============================] - 6s 264us/step - loss: 0.1505\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 6s 264us/step - loss: 0.1372\n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 7s 283us/step - loss: 0.1110\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 7s 283us/step - loss: 0.2274\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 7s 271us/step - loss: 0.1532\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 7s 267us/step - loss: 0.1186\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 7s 271us/step - loss: 0.1663\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 7s 269us/step - loss: 0.0871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137249518>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Building a deep and wide ANN with high number of nodes \"\"\"\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "regressor_2 = Sequential()\n",
    "regressor_2.add(Dense(50, kernel_initializer = 'normal',input_dim = 7))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(100, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(200, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(100, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(50, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(20, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(1, kernel_initializer = 'normal'))\n",
    "\n",
    "\"\"\" Compiling the regressor \"\"\"\n",
    "regressor_2.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "\"\"\" Fitting the Artifilial Neural Network to our training data \"\"\"\n",
    "regressor_2.fit(X_train, y_train, batch_size=5, epochs=100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.101746</td>\n",
       "      <td>0.215436</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.230661</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.276398</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.196210</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.227251</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.425145</td>\n",
       "      <td>1.133756</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.175579</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.235881</td>\n",
       "      <td>-0.228265</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.201183</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.282926</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.264347</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.879658</td>\n",
       "      <td>2.619905</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.222890</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.230967</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.274036</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.232784</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.248643</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.243316</td>\n",
       "      <td>2.024315</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.257086</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.254242</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.248279</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.243316</td>\n",
       "      <td>2.060818</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.248442</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.243268</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.251378</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.252316</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.209419</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.821420</td>\n",
       "      <td>0.772393</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.101746</td>\n",
       "      <td>0.139470</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.266960</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.292528</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.282487</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>0.821420</td>\n",
       "      <td>0.670809</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>-0.235881</td>\n",
       "      <td>-0.259390</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.276768</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.190800</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.230728</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>-0.235881</td>\n",
       "      <td>-0.231982</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.245354</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.274821</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.234573</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0.101746</td>\n",
       "      <td>0.172849</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>-0.235881</td>\n",
       "      <td>-0.260033</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.265799</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6055</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.233138</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.256551</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.182805</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6058</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.286875</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6059</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.213552</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6060</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.234264</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6061</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.255827</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.242487</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6063</th>\n",
       "      <td>16.152226</td>\n",
       "      <td>15.207426</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6064</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.207631</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6065</th>\n",
       "      <td>0.461583</td>\n",
       "      <td>0.420739</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.254495</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.257922</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.266743</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>-0.258092</td>\n",
       "      <td>-0.239815</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>-0.211198</td>\n",
       "      <td>-0.205735</td>\n",
       "      <td>0.772048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6071 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual Values  Predicted Values  Mean Squared Error\n",
       "0          0.101746          0.215436            0.772048\n",
       "1         -0.211198         -0.230661            0.772048\n",
       "2         -0.258092         -0.276398            0.772048\n",
       "3         -0.196210          0.003113            0.772048\n",
       "4         -0.211198         -0.227251            0.772048\n",
       "5          1.425145          1.133756            0.772048\n",
       "6         -0.211198         -0.175579            0.772048\n",
       "7         -0.235881         -0.228265            0.772048\n",
       "8         -0.211198         -0.201183            0.772048\n",
       "9         -0.258092         -0.282926            0.772048\n",
       "10        -0.211198         -0.264347            0.772048\n",
       "11         3.879658          2.619905            0.772048\n",
       "12        -0.211198         -0.222890            0.772048\n",
       "13        -0.211198         -0.230967            0.772048\n",
       "14        -0.258092         -0.274036            0.772048\n",
       "15        -0.211198         -0.232784            0.772048\n",
       "16        -0.211198         -0.248643            0.772048\n",
       "17         2.243316          2.024315            0.772048\n",
       "18        -0.211198         -0.257086            0.772048\n",
       "19        -0.211198         -0.254242            0.772048\n",
       "20        -0.258092         -0.248279            0.772048\n",
       "21         2.243316          2.060818            0.772048\n",
       "22        -0.258092         -0.248442            0.772048\n",
       "23        -0.211198         -0.243268            0.772048\n",
       "24        -0.211198         -0.251378            0.772048\n",
       "25        -0.211198         -0.252316            0.772048\n",
       "26        -0.211198         -0.209419            0.772048\n",
       "27         0.821420          0.772393            0.772048\n",
       "28         0.101746          0.139470            0.772048\n",
       "29        -0.258092         -0.266960            0.772048\n",
       "...             ...               ...                 ...\n",
       "6041      -0.211198         -0.292528            0.772048\n",
       "6042      -0.258092         -0.282487            0.772048\n",
       "6043       0.821420          0.670809            0.772048\n",
       "6044      -0.235881         -0.259390            0.772048\n",
       "6045      -0.258092         -0.276768            0.772048\n",
       "6046      -0.211198         -0.190800            0.772048\n",
       "6047      -0.211198         -0.230728            0.772048\n",
       "6048      -0.235881         -0.231982            0.772048\n",
       "6049      -0.211198         -0.245354            0.772048\n",
       "6050      -0.258092         -0.274821            0.772048\n",
       "6051      -0.211198         -0.234573            0.772048\n",
       "6052       0.101746          0.172849            0.772048\n",
       "6053      -0.235881         -0.260033            0.772048\n",
       "6054      -0.211198         -0.265799            0.772048\n",
       "6055      -0.211198         -0.233138            0.772048\n",
       "6056      -0.258092         -0.256551            0.772048\n",
       "6057      -0.211198         -0.182805            0.772048\n",
       "6058      -0.258092         -0.286875            0.772048\n",
       "6059      -0.211198         -0.213552            0.772048\n",
       "6060      -0.258092         -0.234264            0.772048\n",
       "6061      -0.211198         -0.255827            0.772048\n",
       "6062      -0.258092         -0.242487            0.772048\n",
       "6063      16.152226         15.207426            0.772048\n",
       "6064      -0.211198         -0.207631            0.772048\n",
       "6065       0.461583          0.420739            0.772048\n",
       "6066      -0.258092         -0.254495            0.772048\n",
       "6067      -0.211198         -0.257922            0.772048\n",
       "6068      -0.258092         -0.266743            0.772048\n",
       "6069      -0.258092         -0.239815            0.772048\n",
       "6070      -0.211198         -0.205735            0.772048\n",
       "\n",
       "[6071 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Predicting the values for Helpful Votes on the test data \"\"\"\n",
    "y_pred_2 = regressor_2.predict(X_test)\n",
    "\n",
    "\"\"\" Creating a dataframe to compare the actual values against predicted values \"\"\"\n",
    "y_pred_2 = y_pred_2.reshape(6091,)\n",
    "temp_2 = {'Actual Values': y_test,'Predicted Values': y_pred_2}\n",
    "y_compare_2 = pd.DataFrame(temp_2)\n",
    "\n",
    "\"\"\" Calculating the Mean Squared Error to estimate the efficiency of the ANN\"\"\"\n",
    "y_compare_2['Mean Squared Error'] = ((np.diff(y_compare_1.values) ** 2).mean() ** .5)\n",
    "y_compare_2.head(-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
